{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ibu dan bapak.. code yang saya upload outputnya dihapus agar dapat diupload di Github. berikut versi link dengan output:\n",
        "\n",
        "https://colab.research.google.com/drive/1TSiB6sJF6hi-aLhIKHU1skn_vGjqjAri?usp=sharing\n",
        "\n",
        "disclaimer juga..  karena saya run berulang kembali menggunakan gpu, jadi saya run code ini di akun lain agar tidak perlu menggunakan colab pro.. izin bapak dan ibu ðŸ™ðŸ»\n"
      ],
      "metadata": {
        "id": "dyu7ay_SatVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch, Torchvision, and YOLOv5 dependencies\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118  # CUDA 11.8\n",
        "!pip install opencv-python-headless  # For video processing\n",
        "!pip install ultralytics  # YOLOv5 by Ultralytics\n",
        "\n",
        "# Enable GPU acceleration in Colab under 'Runtime' > 'Change runtime type' > 'Hardware accelerator' > 'GPU'\n"
      ],
      "metadata": {
        "id": "hN4JMTb02dNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VYkFliBkwE53zIvsRU0P3A6LpIe-zJ-C"
      ],
      "metadata": {
        "id": "UaVlXO1p7vON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = ('/content/streetwalk.mov')"
      ],
      "metadata": {
        "id": "GgqD7mvj_J98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the pre-trained YOLOv5 model (e.g., YOLOv5s)\n",
        "model = YOLO('yolov5s.pt')  # 's' stands for small model, optimized for speed"
      ],
      "metadata": {
        "id": "S4cr_mUgAQZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/streetwalk.mov'  # Update with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Process frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO object detection on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Extract detection details\n",
        "    for detection in results:\n",
        "        boxes = detection.boxes\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0]  # Bounding box coordinates\n",
        "            label = box.cls  # Class label\n",
        "            confidence = box.conf  # Confidence score\n",
        "            print(f\"Detected {label} with confidence {confidence}\")\n",
        "\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "ArNmmXVo8zYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the pre-trained YOLOv5 model (e.g., YOLOv5s)\n",
        "model = YOLO('yolov5su.pt')  # 's' stands for small model, optimized for speed\n",
        "\n",
        "detections = []  # To store processed detection results\n",
        "\n",
        "# Example preprocessing function\n",
        "def preprocess_detections(results):\n",
        "    for detection in results:\n",
        "        boxes = detection.boxes\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Convert to integers\n",
        "            label = box.cls\n",
        "            confidence = box.conf\n",
        "            detections.append({\n",
        "                \"label\": label,\n",
        "                \"confidence\": confidence,\n",
        "                \"bbox\": [x1, y1, x2, y2]\n",
        "            })\n",
        "\n",
        "# Load video\n",
        "video_path = '/content/streetwalk.mov'  # Update with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Process frames and apply preprocessing within the loop\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Run YOLO object detection on the frame\n",
        "    results = model(frame)\n",
        "\n",
        "    # Apply preprocessing for each frame's detections\n",
        "    preprocess_detections(results)\n",
        "\n",
        "cap.release()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q-pav11xJAh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple example of tracking based on previous detections\n",
        "previous_detections = {}\n",
        "\n",
        "def track_objects(detections):\n",
        "    tracked_objects = []\n",
        "    for det in detections:\n",
        "        # Logic to assign consistent IDs to each detected object\n",
        "        # For simplicity, we assume previous_detections exists with same structure\n",
        "        for prev in previous_detections:\n",
        "            if some_tracking_condition(det, prev):\n",
        "                tracked_objects.append(det)  # Update tracking logic as needed\n",
        "    return tracked_objects\n"
      ],
      "metadata": {
        "id": "hNv-aTS93D_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow  # Colab-specific display function\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the YOLOv5 model and move it to the specified device\n",
        "model = YOLO(\"yolov5su.pt\")  # Small YOLOv5 model for faster inference\n",
        "model.to(device)\n",
        "\n",
        "# Set model confidence and IoU thresholds for optimization\n",
        "model.conf = 0.4  # Confidence threshold to filter weak detections\n",
        "model.iou = 0.45  # Intersection over Union (IoU) threshold for non-max suppression\n",
        "\n",
        "# Set video source\n",
        "video_path = '/content/streetwalk.mov'  # Replace with your video file path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Reduce frame resolution to optimize speed\n",
        "width, height = 640, 480\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "# Define display size for visualization (smaller size)\n",
        "display_width, display_height = 320, 240  # Adjust to smaller dimensions\n",
        "\n",
        "# Process every nth frame to reduce workload\n",
        "frame_skip = 2  # Adjust frame skip for balance between speed and accuracy\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Skip frames based on frame_skip value\n",
        "    if frame_count % frame_skip == 0:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform inference on the frame\n",
        "        results = model(frame, device=device)\n",
        "\n",
        "        # Process results\n",
        "        for detection in results:\n",
        "          boxes = detection.boxes\n",
        "          for box in boxes:\n",
        "              x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
        "              label = box.cls.item()  # Get the class label as an integer using .item()\n",
        "              confidence = box.conf.item()  # Get confidence score as a float using .item()\n",
        "\n",
        "              # Draw bounding box and label on frame\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "              cv2.putText(frame, f\"{int(label)}: {confidence:.2f}\", (x1, y1 - 10),  # Format label as integer\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "        # Calculate FPS and display it on the frame\n",
        "        end_time = time.time()\n",
        "        fps = 1 / (end_time - start_time)\n",
        "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Resize frame for a smaller display\n",
        "        display_frame = cv2.resize(frame, (display_width, display_height))\n",
        "\n",
        "        # Display the frame with detections\n",
        "        cv2_imshow(display_frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "SnIvRIVXDpMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set video source\n",
        "video_path = '/content/streetwalk.mov'  # Replace with your video file path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the video properties\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Initialize VideoWriter to save the output video with detections\n",
        "output_path = '/content/output_with_detections2.mp4'  # Output video path\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Process every nth frame to reduce workload\n",
        "frame_skip = 2  # Adjust frame skip for balance between speed and accuracy\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Skip frames based on frame_skip value\n",
        "    if frame_count % frame_skip == 0:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform inference on the frame\n",
        "        results = model(frame, device=device)\n",
        "\n",
        "        # Process results\n",
        "        for detection in results:\n",
        "            boxes = detection.boxes\n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
        "                label = box.cls.item()  # Get the class label as an integer using .item()\n",
        "                confidence = box.conf.item()  # Get confidence score as a float using .item()\n",
        "\n",
        "                # Draw bounding box and label on frame\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                cv2.putText(frame, f\"{int(label)}: {confidence:.2f}\", (x1, y1 - 10),  # Format label as integer\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Calculate FPS and display it on the frame\n",
        "        end_time = time.time()\n",
        "        fps = 1 / (end_time - start_time)\n",
        "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "# Release video capture and writer resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Processed video saved at: {output_path}\")"
      ],
      "metadata": {
        "id": "IMm0CvVKt-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path"
      ],
      "metadata": {
        "id": "kUP4OBYgyRqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = '/content/output_with_detections.mp4'  # Output video path\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))"
      ],
      "metadata": {
        "id": "26Gp-JKpuWhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.write(frame)"
      ],
      "metadata": {
        "id": "uhtjPREcuZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.release()"
      ],
      "metadata": {
        "id": "rhpDDIvOuavg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_path)"
      ],
      "metadata": {
        "id": "dg3V3MMPudcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil yang ditampilkan dalam tangkapan layar sudah cukup baik dan sesuai dengan output yang diharapkan dari model YOLO. Berikut penjelasannya:\n",
        "\n",
        "1. **Objek yang Terdeteksi**:\n",
        "   - Model mendeteksi berbagai objek seperti \"persons\" (orang), \"car\" (mobil), \"bus\" (bis), \"backpacks\" (tas ransel), dan \"handbag\" (tas tangan), dengan bounding box yang ditampilkan di sekitar masing-masing objek. Ini menunjukkan bahwa model mampu mengidentifikasi dan mengklasifikasikan banyak objek di area yang ramai, yang merupakan fitur utama YOLO.\n",
        "\n",
        "2. **Bounding Box**:\n",
        "   - Bounding box ditampilkan dengan akurat di sekitar objek yang terdeteksi, membantu menentukan lokasi setiap objek dalam frame. Ini adalah elemen penting dalam tugas deteksi objek.\n",
        "\n",
        "3. **Kecepatan Inference (Prediksi)**:\n",
        "   - Setiap frame menampilkan rincian waktu pemrosesan:\n",
        "     - **Waktu preprocess**: Waktu yang dibutuhkan untuk mempersiapkan frame untuk YOLO.\n",
        "     - **Waktu inference**: Waktu yang dibutuhkan YOLO untuk mendeteksi objek di dalam frame.\n",
        "     - **Waktu postprocess**: Waktu untuk mempersiapkan output agar dapat divisualisasikan.\n",
        "   - Waktu pemrosesan keseluruhan (misalnya, ~11,4 ms per frame) juga ditampilkan, memberikan gambaran tentang performa real-time.\n",
        "\n",
        "4. **Resolusi Gambar**:\n",
        "   - Resolusi 384x640 cukup baik untuk keseimbangan antara kecepatan dan kualitas deteksi. Ukuran ini umumnya cocok untuk aplikasi real-time atau hampir real-time.\n",
        "\n",
        "Secara keseluruhan, output ini menunjukkan bahwa model bekerja dengan baik dalam hal deteksi objek, kecepatan, dan akurasi. Jika waktu pemrosesan dan kualitas deteksi sudah memenuhi kebutuhan proyek Anda, maka pengaturan ini sudah mencukupi. Namun, Anda masih bisa bereksperimen dengan optimisasi model jika memerlukan FPS yang lebih tinggi atau jika ada persyaratan kinerja tertentu.\""
      ],
      "metadata": {
        "id": "VK1vJOpuHynA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utIuypDMaWnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VIDEO"
      ],
      "metadata": {
        "id": "LmM1TCxmHWvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 16CVtEieGWnpFy1yQtCcgJaVWTWIZdWM5"
      ],
      "metadata": {
        "id": "bhJrtbSmHV3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the YOLOv5 model and move it to the specified device\n",
        "model = YOLO(\"yolov5su.pt\")  # Small YOLOv5 model for faster inference\n",
        "model.to(device)\n",
        "\n",
        "# Set model confidence and IoU thresholds for optimization\n",
        "model.conf = 0.4  # Confidence threshold to filter weak detections\n",
        "model.iou = 0.45  # Intersection over Union (IoU) threshold for non-max suppression\n",
        "\n",
        "# Set video source\n",
        "video_path2 = '/content/5743518-hd_1920_1080_30fps.mp4'  # Replace with your video file path\n",
        "cap2 = cv2.VideoCapture(video_path2)\n",
        "\n",
        "# Reduce frame resolution to optimize speed\n",
        "width, height = 640, 480\n",
        "cap2.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
        "cap2.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "\n",
        "# Define display size for visualization (smaller size)\n",
        "display_width, display_height = 320, 240  # Adjust to smaller dimensions\n",
        "\n",
        "# Process every nth frame to reduce workload\n",
        "frame_skip = 2  # Adjust frame skip for balance between speed and accuracy\n",
        "frame_count = 0\n",
        "\n",
        "while cap2.isOpened():\n",
        "    ret, frame = cap2.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Skip frames based on frame_skip value\n",
        "    if frame_count % frame_skip == 0:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform inference on the frame\n",
        "        results = model(frame, device=device)\n",
        "\n",
        "        # Process results\n",
        "        for detection in results:\n",
        "          boxes = detection.boxes\n",
        "          for box in boxes:\n",
        "              x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
        "              label = box.cls.item()  # Get the class label as an integer using .item()\n",
        "              confidence = box.conf.item()  # Get confidence score as a float using .item()\n",
        "\n",
        "              # Draw bounding box and label on frame\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "              cv2.putText(frame, f\"{int(label)}: {confidence:.2f}\", (x1, y1 - 10),  # Format label as integer\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "        # Calculate FPS and display it on the frame\n",
        "        end_time = time.time()\n",
        "        fps = 1 / (end_time - start_time)\n",
        "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Resize frame for a smaller display\n",
        "        display_frame = cv2.resize(frame, (display_width, display_height))\n",
        "\n",
        "        # Display the frame with detections\n",
        "        cv2_imshow(display_frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "SF6QkjHrHY9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set video source\n",
        "video_path2 = '/content/5743518-hd_1920_1080_30fps.mp4'  # Replace with your video file path\n",
        "cap2 = cv2.VideoCapture(video_path2)\n",
        "\n",
        "# Get the video properties\n",
        "fps = cap2.get(cv2.CAP_PROP_FPS)\n",
        "frame_width = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Initialize VideoWriter to save the output video with detections\n",
        "output_path2 = '/content/output_3.mp4'  # Output video path\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
        "out = cv2.VideoWriter(output_path2, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Process every nth frame to reduce workload\n",
        "frame_skip = 2  # Adjust frame skip for balance between speed and accuracy\n",
        "frame_count = 0\n",
        "\n",
        "while cap2.isOpened():\n",
        "    ret, frame = cap2.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Skip frames based on frame_skip value\n",
        "    if frame_count % frame_skip == 0:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform inference on the frame\n",
        "        results = model(frame, device=device)\n",
        "\n",
        "        # Process results\n",
        "        for detection in results:\n",
        "            boxes = detection.boxes\n",
        "            for box in boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
        "                label = box.cls.item()  # Get the class label as an integer using .item()\n",
        "                confidence = box.conf.item()  # Get confidence score as a float using .item()\n",
        "\n",
        "                # Draw bounding box and label on frame\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                cv2.putText(frame, f\"{int(label)}: {confidence:.2f}\", (x1, y1 - 10),  # Format label as integer\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "        # Calculate FPS and display it on the frame\n",
        "        end_time = time.time()\n",
        "        fps = 1 / (end_time - start_time)\n",
        "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "# Release video capture and writer resources\n",
        "cap2.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(f\"Processed video saved at: {output_path2}\")"
      ],
      "metadata": {
        "id": "AR_geRmAHo1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(output_path2)"
      ],
      "metadata": {
        "id": "ZN6UYrRiJrO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torchvision.ops import nms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Initialize variables\n",
        "detection_results = []\n",
        "frame_index = 0\n",
        "conf_threshold = 0.25\n",
        "iou_threshold = 0.6\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap2.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert frame to RGB for YOLO\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # YOLO inference\n",
        "    results = model(frame_rgb)\n",
        "    bbox_data = results.xyxy[0].cpu().numpy()\n",
        "    boxes, scores, labels = bbox_data[:, :4], bbox_data[:, 4], bbox_data[:, 5]\n",
        "\n",
        "    # NMS to filter boxes\n",
        "    boxes_tensor = torch.tensor(boxes)\n",
        "    scores_tensor = torch.tensor(scores)\n",
        "    indices = nms(boxes_tensor, scores_tensor, iou_threshold)\n",
        "    filtered_boxes = bbox_data[indices]\n",
        "\n",
        "    # Draw detections\n",
        "    for bbox in filtered_boxes:\n",
        "        x1, y1, x2, y2, confidence, cls = bbox\n",
        "        if confidence >= conf_threshold:\n",
        "            label = model.names[int(cls)]\n",
        "            color = (0, 255, 0)\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "            cv2.putText(frame, f'{label} {confidence:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "\n",
        "            # Save results\n",
        "            height, width, _ = frame.shape\n",
        "            detection_results.append({\n",
        "                'frame': frame_index,\n",
        "                'x1': x1 / width,\n",
        "                'y1': y1 / height,\n",
        "                'x2': x2 / width,\n",
        "                'y2': y2 / height,\n",
        "                'confidence': confidence,\n",
        "                'class': label\n",
        "            })\n",
        "\n",
        "    frame_index += 1\n",
        "    out.write(frame)\n",
        "\n",
        "cap2.release()\n",
        "out.release()\n"
      ],
      "metadata": {
        "id": "SL_XpECSMoHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_detections = pd.DataFrame(detection_results)\n",
        "df_detections.to_csv('object_detection_results.csv', index=False)"
      ],
      "metadata": {
        "id": "hDHXOjRLOFFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUI5ZHAraf2e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}